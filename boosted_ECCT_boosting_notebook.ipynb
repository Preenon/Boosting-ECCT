{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "4f56caf1",
      "metadata": {
        "id": "4f56caf1"
      },
      "source": [
        "\n",
        "# Boosted ECCT: Boosting Notebook\n",
        "**Purpose:** Load ECCT baseline models generated by the Training Notebook, collect uncorrected (UC) vectors, perform boosted post-stage fine-tuning, run block-wise schedules, evaluate BER/FER, and (optionally) export Uncor.txt.\n",
        "\n",
        "**Instructions:** Switch the Colab runtime to GPU if possible (otherwise, switch 'device' in CONFIG to 'cpu'). Edit the CONFIG cell below before running any cells to select the code (LDPC or POLAR), blocklength, and other hyperparameters. Then run the notebook from top to bottom. Note that his notebook is configured to boost one transformer model at a time for a specific code, such as for LDPC(121, 60) or for POLAR(64, 32).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1e54b5b",
      "metadata": {
        "id": "f1e54b5b"
      },
      "outputs": [],
      "source": [
        "# CONFIG --- edit these values before running ---\n",
        "\n",
        "from datetime import datetime\n",
        "RUN_TIMESTAMP = datetime.now().strftime(\"%d%b%Y_%H%M\")\n",
        "\n",
        "# Define CONFIG with fixed and independent values\n",
        "CONFIG = {\n",
        "    'code_type': 'LDPC',\n",
        "    'code_n': 121,\n",
        "    'code_k': 60,\n",
        "\n",
        "    'ecct_repo': 'https://github.com/yoniLc/ECCT.git',\n",
        "    'boost_repo': 'https://github.com/ghy1228/LDPC_Error_Floor.git',\n",
        "\n",
        "    'epochs_base': 50,\n",
        "    'epochs_post': 40,\n",
        "    'target_uc': 3000,\n",
        "    'uc_snr_db': 4.5,\n",
        "    'eval_ebno_list': [1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0, 4.5, 5.0, 5.5, 6.0, 6.5, 7.0, 7.5, 8.0],\n",
        "    'batch_size': 128,\n",
        "    'uc_batch_size': 512,\n",
        "    'post_batch_size': 64,\n",
        "    'seed': 42,\n",
        "    'device': 'cuda'\n",
        "}\n",
        "\n",
        "# Calculate 'workdir' after 'code_type', 'code_n', and 'code_k' are defined\n",
        "# Workdir should be unique per experiment to avoid UC mixing\n",
        "CONFIG['workdir'] = f\"/content/{CONFIG['code_type']}_n{CONFIG['code_n']}_k{CONFIG['code_k']}_{RUN_TIMESTAMP}\"\n",
        "\n",
        "print(\"CONFIG:\", CONFIG)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "T1VCzY5YPwUv",
      "metadata": {
        "id": "T1VCzY5YPwUv"
      },
      "source": [
        "## Setup: Drive Mount, Clone Repos, and Locate Baseline Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "uSB0l0wlPwUw",
      "metadata": {
        "id": "uSB0l0wlPwUw"
      },
      "outputs": [],
      "source": [
        "import os, sys, subprocess, glob, shutil\n",
        "import torch, numpy as np, matplotlib.pyplot as plt\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from pathlib import Path\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "drive_base_folder = \"/content/drive/MyDrive/Colab_Models\"\n",
        "\n",
        "# Create temporary local workdir and clone repos\n",
        "workdir = CONFIG['workdir']\n",
        "os.makedirs(workdir, exist_ok=True)\n",
        "os.chdir(workdir)\n",
        "\n",
        "for repo_name, repo_url in [('ECCT', CONFIG['ecct_repo']), ('LDPC_Error_Floor', CONFIG['boost_repo'])]:\n",
        "    if not os.path.isdir(repo_name):\n",
        "        print(f'Cloning {repo_name}...')\n",
        "        subprocess.run(['git', 'clone', repo_url, repo_name], check=False, stdout=subprocess.DEVNULL)\n",
        "\n",
        "print('Installing Python packages...')\n",
        "subprocess.run([sys.executable, '-m', 'pip', 'install', '-q', 'einops', 'tqdm', 'matplotlib', 'scikit-learn'], check=False)\n",
        "\n",
        "# Add ECCT directory to Python path for imports\n",
        "sys.path.insert(0, os.path.join(workdir, 'ECCT'))\n",
        "\n",
        "# Locate the Baseline Model on Drive\n",
        "model_prefix = f\"{CONFIG['code_type']}_n{CONFIG['code_n']}_k{CONFIG['code_k']}\"\n",
        "search_pattern = os.path.join(drive_base_folder, f\"{model_prefix}*_ECCT_Boosted_Results\")\n",
        "\n",
        "# Find all matching Drive root folders (e.g., LDPC_n121_k60_...) and sort by creation time (newest first)\n",
        "all_model_dirs = sorted(\n",
        "    glob.glob(search_pattern),\n",
        "    key=os.path.getmtime,\n",
        "    reverse=True\n",
        ")\n",
        "\n",
        "if not all_model_dirs:\n",
        "    raise RuntimeError(\n",
        "        f\"Could not find any baseline model folder on Drive with prefix: {model_prefix}\"\n",
        "    )\n",
        "\n",
        "# The top-level folder on Drive (e.g., LDPC_n121_k60_18Nov2025_1424_ECCT_Boosted_Results)\n",
        "drive_root_folder = all_model_dirs[0]\n",
        "\n",
        "# Locate the specific model run directory within the root folder.\n",
        "# The target directory starts with the code type and contains the 'best_model'.\n",
        "\n",
        "code_sub_folder_prefix = f\"{CONFIG['code_type'].upper()}__Code\"\n",
        "\n",
        "# Search for folders matching the code prefix directly inside the drive_root_folder\n",
        "code_sub_folder_search = glob.glob(os.path.join(drive_root_folder, f\"{code_sub_folder_prefix}*\"))\n",
        "code_sub_folder_search = list(filter(os.path.isdir, code_sub_folder_search)) # Filter to ensure only directories are included\n",
        "\n",
        "if not code_sub_folder_search:\n",
        "    raise RuntimeError(f\"Could not find the expected code sub-folder with prefix '{code_sub_folder_prefix}' in {drive_root_folder}\")\n",
        "\n",
        "# Select the most recently created code sub-folder as the baseline model directory\n",
        "baseline_model_dir = sorted(\n",
        "    code_sub_folder_search,\n",
        "    key=os.path.getmtime,\n",
        "    reverse=True\n",
        ")[0]\n",
        "\n",
        "baseline_model_path = os.path.join(baseline_model_dir, \"best_model\")\n",
        "\n",
        "if not os.path.exists(baseline_model_path):\n",
        "    raise RuntimeError(f\"'best_model' not found at expected location: {baseline_model_path}\")\n",
        "\n",
        "print(\"\\n--- Baseline Model Loaded ---\")\n",
        "print(\"Drive Root Folder:\", drive_root_folder)\n",
        "print(\"Baseline Model Path:\", baseline_model_path)\n",
        "\n",
        "# Define the model_dir to use for saving local and Drive copies\n",
        "local_model_dir = os.path.join(workdir, 'ECCT', os.path.basename(baseline_model_dir))\n",
        "os.makedirs(local_model_dir, exist_ok=True)\n",
        "\n",
        "# Copy baseline model to local workdir for processing\n",
        "local_baseline_model_path = os.path.join(local_model_dir, 'best_model')\n",
        "shutil.copy(baseline_model_path, local_baseline_model_path)\n",
        "print(\"Baseline model copied locally for processing.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "TZBnDoHcP_8O",
      "metadata": {
        "id": "TZBnDoHcP_8O"
      },
      "source": [
        "## Core Imports and Code Object Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0HqaPr-2P_8P",
      "metadata": {
        "id": "0HqaPr-2P_8P"
      },
      "outputs": [],
      "source": [
        "# Change directory to ECCT repo for local file access\n",
        "os.chdir(os.path.join(CONFIG['workdir'], 'ECCT'))\n",
        "\n",
        "# Core Imports from ECCT\n",
        "from Main import ECC_Dataset, EbN0_to_std, set_seed\n",
        "from Codes import Get_Generator_and_Parity, bin_to_sign, sign_to_bin\n",
        "from Model import ECC_Transformer\n",
        "import torch.serialization\n",
        "from torch import nn\n",
        "\n",
        "# Setup code object\n",
        "class CodeObj: pass\n",
        "code = CodeObj(); code.code_type = CONFIG['code_type']; code.k = CONFIG['code_k']; code.n = CONFIG['code_n']\n",
        "G, H = Get_Generator_and_Parity(code, standard_form=False)\n",
        "code.generator_matrix = torch.from_numpy(G).transpose(0,1).long()\n",
        "code.pc_matrix = torch.from_numpy(H).long()\n",
        "set_seed(CONFIG['seed'])\n",
        "\n",
        "device = torch.device(CONFIG['device'] if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "torch.serialization.add_safe_globals([ECC_Transformer])\n",
        "\n",
        "# Load the baseline model (it will be copied for each sweep)\n",
        "baseline_model = torch.load(local_baseline_model_path,\n",
        "                           map_location='cpu',\n",
        "                           weights_only=False)\n",
        "model = baseline_model # Explicitly assign to 'model' for consistency with later cells\n",
        "\n",
        "print(\"Core imports and code object setup complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "16f69ddf",
      "metadata": {
        "id": "16f69ddf"
      },
      "source": [
        "## Sanity check: show key ECCT files (Main.py, Model.py, Codes.py)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16c6a46d",
      "metadata": {
        "id": "16c6a46d"
      },
      "outputs": [],
      "source": [
        "\n",
        "# List key files and print top of Main.py to confirm flags\n",
        "os.chdir(CONFIG['workdir'] + '/ECCT')\n",
        "print('Current dir:', os.getcwd())\n",
        "!ls -la Main.py Model.py Codes.py || true\n",
        "print('\\n--- Main.py head ---\\n')\n",
        "!sed -n '1,240p' Main.py\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "FtH3_Ia4V2nd",
      "metadata": {
        "id": "FtH3_Ia4V2nd"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "2f76c56f",
      "metadata": {
        "id": "2f76c56f"
      },
      "source": [
        "## Helper: locate the saved baseline model (Main.py saves `best_model` under Results_ECCT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d6c0a50",
      "metadata": {
        "id": "0d6c0a50"
      },
      "outputs": [],
      "source": [
        "# The baseline model has already been loaded from Drive and copied to a local directory.\n",
        "# We need to ensure `model_dir` and `model_path` point to this local copy for subsequent steps.\n",
        "# These variables (`local_model_dir`, `local_baseline_model_path`) were established in\n",
        "# the 'Setup' section cells and should already be available.\n",
        "\n",
        "# Assign the global 'model_dir' and 'model_path' to these pre-determined local paths.\n",
        "model_dir = local_model_dir # e.g., /content/workdir/ECCT/LDPC__Code_n_121_k_60__...\n",
        "model_path = local_baseline_model_path # e.g., /content/workdir/ECCT/LDPC__Code_n_121_k_60__.../best_model\n",
        "\n",
        "# drive_base_folder and RUN_TIMESTAMP are already defined from CONFIG cell and setup cell.\n",
        "# drive_destination_folder is assmebled here for saving new results to Drive.\n",
        "drive_destination_folder = os.path.join(\n",
        "    drive_base_folder,\n",
        "    f\"{CONFIG['code_type']}_n{CONFIG['code_n']}_k{CONFIG['code_k']}_{RUN_TIMESTAMP}_ECCT_Boosted_Results\"\n",
        ")\n",
        "\n",
        "print(\"Using local model directory copied from Drive:\", model_dir)\n",
        "print(\"Model path:\", model_path, \"Exists?\", os.path.exists(model_path))\n",
        "print(\"Drive destination folder for current run results:\", drive_destination_folder)\n",
        "\n",
        "if not os.path.exists(model_path):\n",
        "    raise RuntimeError(\n",
        "        f\"'best_model' not found at expected local location: {model_path}. \"\n",
        "        \"This indicates an issue during the initial model copy from Google Drive.\"\n",
        "    )\n",
        "\n",
        "model_dir  # Show path for notebook cell output for verification purposes"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "59885794",
      "metadata": {
        "id": "59885794"
      },
      "source": [
        "## UC collection using ECCT dataset and saved model\n",
        "This uses `ECC_Dataset` from Main.py and ECCT model API (forward + loss). It saves failing samples to a local `ECCT_uc_dataset/` folder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a62b39e",
      "metadata": {
        "id": "6a62b39e"
      },
      "outputs": [],
      "source": [
        "\n",
        "# UC collection\n",
        "os.chdir(os.path.join(CONFIG['workdir'], 'ECCT'))\n",
        "set_seed(CONFIG['seed'])\n",
        "\n",
        "model.to(device).eval()\n",
        "print(f\"Using existing model for UC collection, moved to device: {device}\")\n",
        "\n",
        "# Recreate code object & parity/generator matrices using Codes.py helper\n",
        "class CodeObj: pass\n",
        "code = CodeObj(); code.code_type = CONFIG['code_type']; code.k = CONFIG['code_k']; code.n = CONFIG['code_n']\n",
        "G, H = Get_Generator_and_Parity(code, standard_form=False)\n",
        "code.generator_matrix = torch.from_numpy(G).transpose(0,1).long()\n",
        "code.pc_matrix = torch.from_numpy(H).long()\n",
        "\n",
        "sigma = EbN0_to_std(CONFIG['uc_snr_db'], code.k / code.n)\n",
        "dataset = ECC_Dataset(code, [sigma], len=CONFIG['target_uc'] * 4, zero_cw=False)\n",
        "loader = DataLoader(dataset, batch_size=CONFIG['uc_batch_size'], shuffle=False, num_workers=2)\n",
        "\n",
        "uc_dir = Path(os.path.join(CONFIG['workdir'], 'ECCT_uc_dataset'))\n",
        "uc_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "collected = 0; TARGET = CONFIG['target_uc']\n",
        "from Codes import bin_to_sign as codes_bin_to_sign\n",
        "for batch_idx, (m, x, z, y, magnitude, syndrome) in enumerate(loader):\n",
        "    magnitude = magnitude.to(device); syndrome = syndrome.to(device); y = y.to(device); x = x.to(device)\n",
        "    with torch.no_grad():\n",
        "        z_pred = model(magnitude, syndrome)\n",
        "        loss, x_pred = model.loss(-z_pred, (y * codes_bin_to_sign(x)).to(device), y)\n",
        "    x_pred_cpu = x_pred.cpu().long(); x_cpu = x.cpu().long()\n",
        "    failed_mask = (x_pred_cpu != x_cpu).any(dim=1)\n",
        "    num_fail = int(failed_mask.sum().item())\n",
        "    if num_fail > 0:\n",
        "        failed_indices = torch.nonzero(failed_mask).squeeze().tolist()\n",
        "        if isinstance(failed_indices, int):\n",
        "            failed_indices = [failed_indices]\n",
        "        for idx_local, i in enumerate(failed_indices):\n",
        "            idx_global = collected + idx_local\n",
        "            # When saving UC samples, ensure canonical format\n",
        "            sample = {\n",
        "                'm': m[i].cpu().long().contiguous(),\n",
        "                'x': x[i].cpu().long().contiguous(),\n",
        "                'y': y[i].cpu().float().contiguous(),\n",
        "                'magnitude': magnitude[i].cpu().float().contiguous(),\n",
        "                'syndrome': syndrome[i].cpu().float().contiguous()\n",
        "            }\n",
        "            torch.save(sample, uc_dir / f'uc_{idx_global}.pt')\n",
        "        collected += num_fail\n",
        "    print(f'Batch {batch_idx}: collected {collected}/{TARGET}', end='\\r')\n",
        "    if collected >= TARGET:\n",
        "        break\n",
        "\n",
        "print('\\nUC collection finished. Saved to', uc_dir)\n",
        "\n",
        "# Copy UC dataset to Google Drive\n",
        "drive_uc_dir = os.path.join(drive_destination_folder, 'ECCT_uc_dataset')\n",
        "try:\n",
        "    shutil.copytree(str(uc_dir), drive_uc_dir, dirs_exist_ok=True)\n",
        "    print(f\"Successfully copied UC dataset to GDrive: {drive_uc_dir}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error copying UC dataset to GDrive: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d547e270",
      "metadata": {
        "id": "d547e270"
      },
      "source": [
        "## Optional: Export Uncor.txt for LDPC_Error_Floor (so TensorFlow post script can be run if desired)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "09c0d5f0",
      "metadata": {
        "id": "09c0d5f0"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Export Uncor.txt compatible with LDPC_Error_Floor expected format\n",
        "uc_dir = Path(os.path.join(CONFIG['workdir'], 'ECCT_uc_dataset'))\n",
        "out_dir = Path(os.path.join(CONFIG['workdir'], 'LDPC_Error_Floor', 'Inputs'))\n",
        "out_dir.mkdir(parents=True, exist_ok=True)\n",
        "uncor_file = out_dir / \"[Uncor]_ecct_uc.txt\"\n",
        "count = 0\n",
        "with open(uncor_file, 'w') as f:\n",
        "    for p in sorted(uc_dir.glob('uc_*.pt')):\n",
        "        d = torch.load(p)\n",
        "        y = d.get('y', None)\n",
        "        if y is None:\n",
        "            continue\n",
        "        y_np = y.numpy().reshape(-1)\n",
        "        # Write header zeros then -y as in Print_Functions.write_uncor_file\n",
        "        row = np.concatenate((np.zeros(3), -y_np))\n",
        "        f.write('\\t'.join([f'{v:.3f}' for v in row]) + '\\n')\n",
        "        count += 1\n",
        "print(f'Wrote {count} lines to', uncor_file)\n",
        "\n",
        "# Assuming 'drive_destination_folder' is defined from the setup cell\n",
        "if 'drive_destination_folder' in locals() or 'drive_destination_folder' in globals():\n",
        "    # Create a subfolder for these inputs in the Drive directory for neatness\n",
        "    drive_uncor_dir = os.path.join(drive_destination_folder, 'LDPC_Error_Floor_Inputs')\n",
        "    os.makedirs(drive_uncor_dir, exist_ok=True)\n",
        "    drive_uncor_file = os.path.join(drive_uncor_dir, os.path.basename(uncor_file))\n",
        "\n",
        "    try:\n",
        "        shutil.copy(uncor_file, drive_uncor_file)\n",
        "        print(f\"Successfully copied Uncor.txt to GDrive: {drive_uncor_file}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error copying Uncor.txt to GDrive: {e}. Check if drive_destination_folder is correctly defined.\")\n",
        "else:\n",
        "    print(\"Skipping GDrive copy: 'drive_destination_folder' variable is missing.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "O4dHDAJWfEpU",
      "metadata": {
        "id": "O4dHDAJWfEpU"
      },
      "outputs": [],
      "source": [
        "# ---------- TRANSFER LEARNING CELL ----------\n",
        "import torch, os\n",
        "from torch.serialization import add_safe_globals\n",
        "\n",
        "# Make sure the ECC_Transformer class is available for safe loads if needed\n",
        "try:\n",
        "    from Model import ECC_Transformer\n",
        "    add_safe_globals([ECC_Transformer])\n",
        "except Exception as e:\n",
        "    print(\"Model class safe-globals not registered (ok if loading state_dict only):\", e)\n",
        "\n",
        "transfer_model_path = None  # <-- set to desired path or None\n",
        "\n",
        "if transfer_model_path and os.path.exists(transfer_model_path):\n",
        "    print(\"Loading pretrained (weights_only=True recommended) for transfer:\", transfer_model_path)\n",
        "    pretrained = torch.load(transfer_model_path, map_location='cpu', weights_only=False)\n",
        "    pre_sd = {}\n",
        "    if hasattr(pretrained, 'state_dict'):\n",
        "        pre_sd = pretrained.state_dict()\n",
        "    else:\n",
        "        # pretrained might be a raw state-dict saved with torch.save(model.state_dict())\n",
        "        pre_sd = pretrained\n",
        "\n",
        "    cur_sd = model.state_dict()\n",
        "    # copy matching keys only\n",
        "    for k in cur_sd.keys():\n",
        "        if k in pre_sd and pre_sd[k].shape == cur_sd[k].shape:\n",
        "            cur_sd[k] = pre_sd[k].clone()\n",
        "    model.load_state_dict(cur_sd)\n",
        "\n",
        "\n",
        "    print(f\"Transfer complete.\")\n",
        "else:\n",
        "    print(\"No transfer model provided or file not found. Skipping transfer.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "AS7gTR-YfJ1N",
      "metadata": {
        "id": "AS7gTR-YfJ1N"
      },
      "outputs": [],
      "source": [
        "# PER-ITERATION SCALAR WEIGHTS\n",
        "# Wrapper around a trained ECC_Transformer baseline model.\n",
        "# It uses small per-layer parameters w_bar (channel), w_sc (satisfied-CN), w_uc (unsatisfied-CN).\n",
        "\n",
        "import torch.nn as nn\n",
        "\n",
        "class ECCT_ScalarShared(nn.Module):\n",
        "    def __init__(self, base_model):\n",
        "        super().__init__()\n",
        "        if isinstance(base_model, ECCT_ScalarShared):\n",
        "            base_model = base_model.base\n",
        "        self.base = base_model\n",
        "\n",
        "        self.src_embed = base_model.src_embed\n",
        "        self.oned_final_embed = base_model.oned_final_embed\n",
        "        self.out_fc = base_model.out_fc\n",
        "        if hasattr(base_model, 'src_mask'):\n",
        "            self.register_buffer('src_mask', base_model.src_mask.clone())\n",
        "\n",
        "        # keep decoder and layers\n",
        "        self.decoder = base_model.decoder\n",
        "        self.decoder_layers = base_model.decoder.layers       # list[EncoderLayer]\n",
        "        self.L = len(self.decoder_layers)\n",
        "\n",
        "\n",
        "        # small per-iteration scalars (initialize to 1)\n",
        "        # paper initializes weights to 1 (so initial decoder is MS-equivalent)\n",
        "        self.w_bar = nn.Parameter(torch.ones(self.L))  # applied to VN (channel) positions\n",
        "        self.w_sc  = nn.Parameter(torch.ones(self.L))  # CN weight for satisfied CNs\n",
        "        self.w_uc  = nn.Parameter(torch.ones(self.L))  # CN weight for unsatisfied CNs\n",
        "\n",
        "    def forward(self, magnitude, syndrome):\n",
        "        # magnitude: (B, n), syndrome: (B, m)\n",
        "        B, n = magnitude.shape\n",
        "        m = syndrome.shape[1]\n",
        "        seq_len = n + m\n",
        "        x = torch.cat([magnitude, syndrome], dim=-1).unsqueeze(-1)  # (B, seq_len, 1)\n",
        "        emb = self.src_embed.unsqueeze(0) * x                      # (B, seq_len, d)\n",
        "        out = emb\n",
        "\n",
        "        for ell in range(self.L):\n",
        "            device = out.device\n",
        "            wm = torch.ones((B, seq_len, 1), dtype=out.dtype, device=device)\n",
        "\n",
        "            # VN scale\n",
        "            wm[:, :n, :] *= float(self.w_bar[ell].item())\n",
        "\n",
        "            # CN mask\n",
        "            cn_mask = (syndrome.abs() > 0).float().unsqueeze(-1)\n",
        "\n",
        "            # SCN default weight\n",
        "            scw = float(self.w_sc[ell].item())\n",
        "            ucw = float(self.w_uc[ell].item())\n",
        "\n",
        "            cn_default = torch.full((B, m, 1), scw, dtype=out.dtype, device=device)\n",
        "            cn_default[cn_mask.bool()] = ucw\n",
        "            wm[:, n:, :] = cn_default\n",
        "\n",
        "            out = out * wm\n",
        "            out = self.decoder_layers[ell](out, self.src_mask)\n",
        "\n",
        "        logits = self.out_fc(self.oned_final_embed(out).squeeze(-1))\n",
        "        return logits\n",
        "    def loss(self, *args, **kwargs):\n",
        "        return self.base.loss(*args, **kwargs)\n",
        "\n",
        "\n",
        "\n",
        "# Replace model\n",
        "wrapped = ECCT_ScalarShared(model)\n",
        "model = wrapped.to(device)  # continue using `model` variable\n",
        "\n",
        "\n",
        "print(\"Dynamic weight sharing applied.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-OiyphU1fNoJ",
      "metadata": {
        "id": "-OiyphU1fNoJ"
      },
      "outputs": [],
      "source": [
        "# ---------- DATA AUGMENTATION / IMPORTANCE SAMPLING ----------\n",
        "from Codes import bin_to_sign, sign_to_bin\n",
        "\n",
        "beta = 0.7   # boosting paper suggests 0.7\n",
        "D = 10       # augmentations per UC vector\n",
        "\n",
        "def augment_uc_vector(magnitude, syndrome, x_true, y, H, sigma=None):\n",
        "    \"\"\"\n",
        "    Inputs: 1-D tensors:\n",
        "      magnitude (n,), syndrome (m,), x_true (n,), y (n,)\n",
        "      H: parity-check matrix (m x n) as torch.LongTensor\n",
        "      sigma: noise std used during UC collection (required for correct AWGN generation)\n",
        "\n",
        "    Returns list of D tuples: (mag_aug, syn_aug, x_true, y_aug)\n",
        "    \"\"\"\n",
        "    if not isinstance(H, torch.Tensor):\n",
        "        H = torch.from_numpy(H).long()\n",
        "\n",
        "    n = magnitude.shape[0]\n",
        "\n",
        "    # Identify error positions E\n",
        "    pred_bits = (torch.sign(y) < 0).long()\n",
        "    E = (pred_bits != x_true.long()).nonzero(as_tuple=False).flatten().tolist()\n",
        "\n",
        "    if sigma is None:\n",
        "        raise ValueError(\"augment_uc_vector now requires sigma (AWGN std).\")\n",
        "\n",
        "    augmented = []\n",
        "    for _ in range(D):\n",
        "\n",
        "        # Generate a fresh AWGN sample\n",
        "        # This is the channel model: y = sign(x) + N(0, sigma^2)\n",
        "        noise = torch.randn_like(y) * sigma\n",
        "        y_aug = bin_to_sign(x_true) + noise\n",
        "\n",
        "        # Apply importance sampling bias toward error positions\n",
        "        if len(E) > 0:\n",
        "            y_aug[E] -= beta\n",
        "\n",
        "        # Rebuild magnitude, bits, and syndrome\n",
        "        mag_aug = torch.abs(y_aug)\n",
        "        bits = sign_to_bin(torch.sign(y_aug)).long().unsqueeze(0)  # (1,n)\n",
        "        syn = (bits @ H.t()) % 2\n",
        "        syn = bin_to_sign(syn.squeeze(0))\n",
        "\n",
        "        augmented.append((mag_aug, syn, x_true.clone(), y_aug))\n",
        "\n",
        "    return augmented\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d6e741f1",
      "metadata": {
        "id": "d6e741f1"
      },
      "source": [
        "## Post-stage fine-tune (PyTorch) on UC dataset\n",
        "This cell fine-tunes the loaded ECCT model on UC samples (adds small FER surrogate); uses block-wise freezing by default."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9721686f",
      "metadata": {
        "id": "9721686f"
      },
      "outputs": [],
      "source": [
        "# POST-STAGE FINE-TUNE â€” BOOSTING + BLOCK-WISE\n",
        "# Assuming a 6-layer ECCT Transformer model is provided (generalized for any L)\n",
        "\n",
        "# ---------- BUILD AUGMENTED UC DATASET ----------\n",
        "uc_dir = Path(os.path.join(CONFIG['workdir'], 'ECCT_uc_dataset'))\n",
        "files = sorted([str(p) for p in uc_dir.glob('uc_*.pt')])\n",
        "print(\"UC files found:\", len(files))\n",
        "if len(files) == 0:\n",
        "    raise RuntimeError(\"No UC samples found; run UC collection first.\")\n",
        "\n",
        "# get H\n",
        "class C: pass\n",
        "code = C()\n",
        "code.code_type = CONFIG['code_type']; code.n = CONFIG['code_n']; code.k = CONFIG['code_k']\n",
        "G, H = Get_Generator_and_Parity(code, standard_form=False)\n",
        "H = torch.from_numpy(H).long()\n",
        "\n",
        "augmented_files = []\n",
        "for f in files:\n",
        "    d = torch.load(f)\n",
        "    mag, syn, x, y = d['magnitude'], d['syndrome'], d['x'], d['y']\n",
        "    sigma = EbN0_to_std(CONFIG['uc_snr_db'], code.k / code.n)\n",
        "\n",
        "    aug_list = augment_uc_vector(mag, syn, x, y, H, sigma)\n",
        "    for j, (mag_aug, syn_aug, x_aug, y_aug) in enumerate(aug_list):\n",
        "        out = {'magnitude': mag_aug, 'syndrome': syn_aug, 'x': x_aug, 'y': y_aug}\n",
        "        out_path = uc_dir / f\"aug_{Path(f).stem}_{j}.pt\"\n",
        "        torch.save(out, out_path)\n",
        "        augmented_files.append(str(out_path))\n",
        "\n",
        "print(\"Augmented UC vectors generated:\", len(augmented_files))\n",
        "\n",
        "# ---------- BLOCK-WISE POST-STAGE FINE-TUNE ----------\n",
        "uc_dir = Path(os.path.join(CONFIG['workdir'], 'ECCT_uc_dataset'))\n",
        "files_uc = sorted([str(p) for p in uc_dir.glob('uc_*.pt')])\n",
        "files_aug = sorted([str(p) for p in uc_dir.glob('aug_*.pt')])\n",
        "files = files_uc + files_aug\n",
        "assert len(files) > 0, \"No UC samples. Run UC collection or augmentation first.\"\n",
        "\n",
        "class UCDataset(Dataset):\n",
        "    def __init__(self, files): self.files = files\n",
        "    def __len__(self): return len(self.files)\n",
        "    def __getitem__(self, idx):\n",
        "        d = torch.load(self.files[idx])\n",
        "        return d['magnitude'], d['syndrome'], d['x'], d['y']\n",
        "\n",
        "def collate_uc(batch):\n",
        "    mags, syns, xs, ys = zip(*batch)\n",
        "    mags = torch.stack([torch.as_tensor(t).float() for t in mags], 0)\n",
        "    syns = torch.stack([torch.as_tensor(t).float() for t in syns], 0)\n",
        "    xs   = torch.stack([torch.as_tensor(t).long()  for t in xs], 0)\n",
        "    ys   = torch.stack([torch.as_tensor(t).float() for t in ys], 0)\n",
        "    return mags, syns, xs, ys\n",
        "\n",
        "ds = UCDataset(files)\n",
        "loader = DataLoader(ds, batch_size=CONFIG['post_batch_size'], shuffle=True, num_workers=0, collate_fn=collate_uc)\n",
        "\n",
        "# block-wise schedule parameters (paper suggests DELTA1=5, DELTA2=10)\n",
        "DELTA1 = 5   # iter_step\n",
        "DELTA2 = 10  # retrain window length\n",
        "ITERS_MAX = len(model.decoder.layers)  # this is L (conceptual mapping)\n",
        "EPOCHS_PER_BLOCK = max(1, CONFIG.get('epochs_post', 3))  # adapt\n",
        "LR = 1e-4\n",
        "\n",
        "training_iter_start = ITERS_MAX // 2\n",
        "training_iter_end   = min(training_iter_start + DELTA1, ITERS_MAX)\n",
        "\n",
        "stages = []\n",
        "while training_iter_start < ITERS_MAX:\n",
        "    retrain_start = max(training_iter_start,\n",
        "                        training_iter_end - DELTA2)\n",
        "    stages.append(list(range(retrain_start, training_iter_end)))\n",
        "\n",
        "    training_iter_start += DELTA1\n",
        "    training_iter_end = min(training_iter_start + DELTA1, ITERS_MAX)\n",
        "\n",
        "print(\"Stages:\", stages)\n",
        "\n",
        "\n",
        "# helper to set trainable layers (freeze others)\n",
        "def set_trainable_layers(model, train_layer_indices):\n",
        "    for i, layer in enumerate(model.decoder.layers):\n",
        "        requires = (i in train_layer_indices)\n",
        "        for p in layer.parameters():\n",
        "            p.requires_grad = requires\n",
        "\n",
        "from Codes import bin_to_sign as codes_bin_to_sign\n",
        "for stage_idx, stage in enumerate(stages):\n",
        "    print(f\"=== Stage {stage_idx+1}/{len(stages)}: training layers {stage} ===\")\n",
        "    set_trainable_layers(model, stage)\n",
        "    opt = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=LR)\n",
        "    for ep in range(EPOCHS_PER_BLOCK):\n",
        "        tot_loss = 0.0\n",
        "        for magnitude, syndrome, x_true, y in loader:\n",
        "            magnitude = magnitude.to(CONFIG['device']); syndrome = syndrome.to(CONFIG['device'])\n",
        "            x_true = x_true.to(CONFIG['device']); y = y.to(CONFIG['device'])\n",
        "            z_mul = (y * codes_bin_to_sign(x_true)).to(CONFIG['device'])\n",
        "            z_pred = model(magnitude, syndrome)\n",
        "            loss_bce, x_pred = model.loss(-z_pred, z_mul, y)\n",
        "            fer_surr = (x_pred != x_true).any(dim=1).float().mean()\n",
        "            loss = fer_surr + 0.01 * loss_bce\n",
        "            opt.zero_grad(); loss.backward(); opt.step()\n",
        "            tot_loss += loss.item() * magnitude.size(0)\n",
        "        print(f\" Stage {stage_idx} epoch {ep+1}/{EPOCHS_PER_BLOCK} avg_loss={tot_loss/len(ds):.4e}\")\n",
        "\n",
        "# final polish: unfreeze all and small lr training\n",
        "for p in model.parameters(): p.requires_grad = True\n",
        "opt = torch.optim.Adam(model.parameters(), lr=LR/10)\n",
        "for ep in range(2):\n",
        "    tot_loss = 0.0\n",
        "    for magnitude, syndrome, x_true, y in loader:\n",
        "        magnitude = magnitude.to(CONFIG['device']); syndrome = syndrome.to(CONFIG['device'])\n",
        "        x_true = x_true.to(CONFIG['device']); y = y.to(CONFIG['device'])\n",
        "        z_mul = (y * codes_bin_to_sign(x_true)).to(CONFIG['device'])\n",
        "        z_pred = model(magnitude, syndrome)\n",
        "        loss_bce, x_pred = model.loss(-z_pred, z_mul, y)\n",
        "        fer_surr = (x_pred != x_true).any(dim=1).float().mean()\n",
        "        loss = fer_surr + 0.01 * loss_bce\n",
        "        opt.zero_grad(); loss.backward(); opt.step()\n",
        "        tot_loss += loss.item() * magnitude.size(0)\n",
        "    print(f\" Final polish epoch {ep+1} avg_loss={tot_loss/len(ds):.4e}\")\n",
        "\n",
        "# Save boosted model\n",
        "boosted_model_path = os.path.join(model_dir, 'boosted_model.pt')\n",
        "torch.save(model, boosted_model_path)\n",
        "print('Saved boosted model to', boosted_model_path)\n",
        "# try copying to drive:\n",
        "try:\n",
        "    shutil.copy(boosted_model_path, os.path.join(drive_destination_folder, 'boosted_model.pt'))\n",
        "    print(\"Copied boosted model to drive.\")\n",
        "except Exception as e:\n",
        "    print(\"Drive copy skipped/error:\", e)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b909bfa7",
      "metadata": {
        "id": "b909bfa7"
      },
      "source": [
        "## Evaluation: BER & FER plots for baseline vs boosted"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d609b6a",
      "metadata": {
        "id": "1d609b6a"
      },
      "outputs": [],
      "source": [
        "from Codes import Get_Generator_and_Parity, bin_to_sign\n",
        "\n",
        "# Prepare code object\n",
        "class CodeObj: pass\n",
        "code = CodeObj(); code.code_type = CONFIG['code_type']; code.k = CONFIG['code_k']; code.n = CONFIG['code_n']\n",
        "G,H = Get_Generator_and_Parity(code, standard_form=False)\n",
        "code.generator_matrix = torch.from_numpy(G).transpose(0,1).long(); code.pc_matrix = torch.from_numpy(H).long()\n",
        "\n",
        "\n",
        "def evaluate_model(model_obj, ebno_list, samples_per_point=3000):\n",
        "    device_eval = torch.device(CONFIG['device'] if torch.cuda.is_available() else 'cpu')\n",
        "    model_obj = model_obj.to(device_eval)\n",
        "    model_obj.eval()\n",
        "\n",
        "    results = {'eb': [], 'BER': [], 'FER': []}\n",
        "\n",
        "    for eb in ebno_list:\n",
        "        sigma = EbN0_to_std(eb, code.k/code.n)\n",
        "        # Assuming ECC_Dataset and DataLoader (with collate_fn) handle variable/fixed code length correctly\n",
        "        ds = ECC_Dataset(code, [sigma], len=samples_per_point, zero_cw=False)\n",
        "\n",
        "        loader = DataLoader(ds, batch_size=CONFIG['batch_size'], shuffle=False, num_workers=0) # Set to 0 for stability\n",
        "\n",
        "        total_bits = 0; bit_errors = 0; total_frames = 0; frame_errors = 0\n",
        "        with torch.no_grad():\n",
        "            # The structure of the yielded batch depends on ECC_Dataset implementation\n",
        "            for m, x, z, y, magnitude, syndrome in loader:\n",
        "                magnitude = magnitude.to(device_eval); syndrome = syndrome.to(device_eval); x = x.to(device_eval)\n",
        "                y = y.to(device_eval) # Move y to the correct device\n",
        "                z_pred = model_obj(magnitude, syndrome)\n",
        "                _, x_pred = model_obj.loss(-z_pred, (y * bin_to_sign(x)), y)\n",
        "                x_pred = x_pred.cpu().long(); x_cpu = x.cpu().long()\n",
        "                total_frames += x_cpu.size(0)\n",
        "                frame_errors += (x_pred != x_cpu).any(dim=1).sum().item()\n",
        "                bit_errors += (x_pred != x_cpu).sum().item()\n",
        "                total_bits += x_cpu.numel()\n",
        "        ber = bit_errors / total_bits; fer = frame_errors / total_frames\n",
        "        results['eb'].append(eb); results['BER'].append(ber); results['FER'].append(fer)\n",
        "        print(f\"Eb/N0={eb}: BER={ber:.3e}, FER={fer:.3e}\")\n",
        "    return results\n",
        "\n",
        "\n",
        "# Load models\n",
        "if 'model_dir' not in locals() and 'model_dir' not in globals() or not model_dir:\n",
        "    print(\"ERROR: 'model_dir' is not defined. Please run the Helper cell first.\")\n",
        "    # Fallback to finding the latest dir if model_dir is somehow lost\n",
        "    res_dirs = sorted(glob.glob(os.path.join(CONFIG['workdir'], 'ECCT', 'Results_ECCT', '*')), key=os.path.getmtime, reverse=True)\n",
        "    if len(res_dirs) > 0:\n",
        "        model_dir = res_dirs[0]\n",
        "    else:\n",
        "        raise EnvironmentError(\"Cannot find 'model_dir' or any results folder.\")\n",
        "\n",
        "\n",
        "baseline_model_path = os.path.join(model_dir, 'best_model')\n",
        "boosted_model_path = os.path.join(model_dir, 'boosted_model.pt')\n",
        "\n",
        "device = torch.device(CONFIG['device'] if torch.cuda.is_available() else 'cpu')\n",
        "print('Loading model from', baseline_model_path, 'to', device)\n",
        "\n",
        "baseline_model = torch.load(baseline_model_path,\n",
        "                           map_location=device,\n",
        "                           weights_only=False)\n",
        "print('Loading model from', boosted_model_path, 'to', device)\n",
        "\n",
        "boosted_model = torch.load(boosted_model_path,\n",
        "                           map_location=device,\n",
        "                           weights_only=False)\n",
        "\n",
        "\n",
        "# Evaluation\n",
        "res_base = evaluate_model(baseline_model, CONFIG['eval_ebno_list'], samples_per_point=2000)\n",
        "res_boost = evaluate_model(boosted_model, CONFIG['eval_ebno_list'], samples_per_point=2000)\n",
        "\n",
        "# --- DATA SAVING (NPZ) ---\n",
        "# Create a dictionary for saving both results\n",
        "data_to_save = {\n",
        "    'baseline_eb': np.array(res_base['eb']),\n",
        "    'baseline_BER': np.array(res_base['BER']),\n",
        "    'baseline_FER': np.array(res_base['FER']),\n",
        "    'boosted_eb': np.array(res_boost['eb']),\n",
        "    'boosted_BER': np.array(res_boost['BER']),\n",
        "    'boosted_FER': np.array(res_boost['FER']),\n",
        "    'config': str(CONFIG) # Save config for context\n",
        "}\n",
        "\n",
        "data_filepath = os.path.join(model_dir, 'evaluation_results.npz')\n",
        "np.savez(data_filepath, **data_to_save)\n",
        "print(f\"Saved evaluation data to: {data_filepath}\")\n",
        "\n",
        "# Ensure the drive destination folder exists before saving plots\n",
        "os.makedirs(drive_destination_folder, exist_ok=True)\n",
        "\n",
        "# --- PLOTTING ---\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.semilogy(res_base['eb'], res_base['BER'], marker='o', label='BER baseline')\n",
        "plt.semilogy(res_boost['eb'], res_boost['BER'], marker='o', label='BER boosted')\n",
        "plt.xlabel('Eb/N0 (dB)'); plt.ylabel('BER'); plt.legend(); plt.grid(True)\n",
        "# Save BER plot to Drive\n",
        "ber_plot_path = os.path.join(drive_destination_folder, 'BER_baseline_vs_boosted.png')\n",
        "plt.savefig(ber_plot_path)\n",
        "print(f\"Saved BER plot to GDrive: {ber_plot_path}\")\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.semilogy(res_base['eb'], res_base['FER'], marker='x', label='FER baseline')\n",
        "plt.semilogy(res_boost['eb'], res_boost['FER'], marker='x', label='FER boosted')\n",
        "plt.xlabel('Eb/N0 (dB)'); plt.ylabel('FER'); plt.legend(); plt.grid(True)\n",
        "# Save FER plot to Drive\n",
        "fer_plot_path = os.path.join(drive_destination_folder, 'FER_baseline_vs_boosted.png')\n",
        "plt.savefig(fer_plot_path)\n",
        "print(f\"Saved FER plot to GDrive: {fer_plot_path}\")\n",
        "plt.show()\n",
        "\n",
        "# --- GDrive Copy for NPZ file ---\n",
        "if 'drive_destination_folder' in locals() or 'drive_destination_folder' in globals():\n",
        "    drive_data_filepath = os.path.join(drive_destination_folder, os.path.basename(data_filepath))\n",
        "    try:\n",
        "        # Copy the newly created .npz file\n",
        "        import shutil\n",
        "        shutil.copy(data_filepath, drive_data_filepath)\n",
        "        print(f\"Successfully copied evaluation data to GDrive: {drive_data_filepath}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error copying evaluation data to GDrive: {e}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}